{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Frorozcoloa/FinancIA/blob/main/Notebooks/0-2%20Train_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "epwoT-bYtT_E",
        "outputId": "6ca47d5d-6034-4887-b5b0-7afa36767baf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.14.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (67.6.1)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.19.1-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=1fe586040326bf9f96111bf07b7c413ea351fb1a092162052b730a81f9e232d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: tokenizers, pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, huggingface-hub, gitdb, transformers, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 huggingface-hub-0.13.4 pathtools-0.1.2 sentry-sdk-1.19.1 setproctitle-1.3.2 smmap-5.0.0 tokenizers-0.13.3 transformers-4.28.1 wandb-0.14.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import set_seed\n",
        "\n",
        "def SEED(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    set_seed(seed)\n",
        "SEED(42)\n",
        "\n",
        "\n",
        "# Configuration by model\n",
        "NUM_VARAIBLES = 3\n",
        "NUM_LABELS = 3\n",
        "num_labels = NUM_LABELS * NUM_VARAIBLES\n",
        "model_name = \"pysentimiento/roberta-es-sentiment\"\n",
        "\n",
        "divice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "HH7wdTX7tX8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eDkRBbStT_H",
        "outputId": "f25accdb-b32e-4feb-bc67-06fb21032265"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-sentiment-analysis and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([9, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([9]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import ( #\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    modelForSequenceClassification,\n",
        "    get_constant_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "# Configuring the model\n",
        "num_labels = NUM_LABELS * NUM_VARAIBLES\n",
        "# model_name = \"pysentimiento/roberta-es-sentiment\"\n",
        "model_name = \"pysentimiento/robertuito-sentiment-analysis\"\n",
        "auto_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, ignore_mismatched_sizes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zY4Uu5CmtT_I",
        "outputId": "fb58f101-80ac-47fa-a72a-6929aa4ed869"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[1;32m----> 3\u001b[0m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mcsv\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mDataset\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39mrain.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\froro\\OneDrive\\Escritorio\\UNAL\\RNA\\FinancIA\\env\\lib\\site-packages\\datasets\\load.py:1767\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1762\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[0;32m   1763\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[0;32m   1764\u001b[0m )\n\u001b[0;32m   1766\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 1767\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   1768\u001b[0m     path\u001b[39m=\u001b[39mpath,\n\u001b[0;32m   1769\u001b[0m     name\u001b[39m=\u001b[39mname,\n\u001b[0;32m   1770\u001b[0m     data_dir\u001b[39m=\u001b[39mdata_dir,\n\u001b[0;32m   1771\u001b[0m     data_files\u001b[39m=\u001b[39mdata_files,\n\u001b[0;32m   1772\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   1773\u001b[0m     features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m   1774\u001b[0m     download_config\u001b[39m=\u001b[39mdownload_config,\n\u001b[0;32m   1775\u001b[0m     download_mode\u001b[39m=\u001b[39mdownload_mode,\n\u001b[0;32m   1776\u001b[0m     revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   1777\u001b[0m     use_auth_token\u001b[39m=\u001b[39muse_auth_token,\n\u001b[0;32m   1778\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[0;32m   1779\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1780\u001b[0m )\n\u001b[0;32m   1782\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
            "File \u001b[1;32mc:\\Users\\froro\\OneDrive\\Escritorio\\UNAL\\RNA\\FinancIA\\env\\lib\\site-packages\\datasets\\load.py:1524\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1521\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg)\n\u001b[0;32m   1523\u001b[0m \u001b[39m# Instantiate the dataset builder\u001b[39;00m\n\u001b[1;32m-> 1524\u001b[0m builder_instance: DatasetBuilder \u001b[39m=\u001b[39m builder_cls(\n\u001b[0;32m   1525\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   1526\u001b[0m     config_name\u001b[39m=\u001b[39mconfig_name,\n\u001b[0;32m   1527\u001b[0m     data_dir\u001b[39m=\u001b[39mdata_dir,\n\u001b[0;32m   1528\u001b[0m     data_files\u001b[39m=\u001b[39mdata_files,\n\u001b[0;32m   1529\u001b[0m     \u001b[39mhash\u001b[39m\u001b[39m=\u001b[39m\u001b[39mhash\u001b[39m,\n\u001b[0;32m   1530\u001b[0m     features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m   1531\u001b[0m     use_auth_token\u001b[39m=\u001b[39muse_auth_token,\n\u001b[0;32m   1532\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[0;32m   1533\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbuilder_kwargs,\n\u001b[0;32m   1534\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1535\u001b[0m )\n\u001b[0;32m   1537\u001b[0m \u001b[39mreturn\u001b[39;00m builder_instance\n",
            "File \u001b[1;32mc:\\Users\\froro\\OneDrive\\Escritorio\\UNAL\\RNA\\FinancIA\\env\\lib\\site-packages\\datasets\\builder.py:376\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[1;34m(self, cache_dir, config_name, hash, base_path, info, features, use_auth_token, repo_id, data_files, data_dir, storage_options, writer_batch_size, name, **config_kwargs)\u001b[0m\n\u001b[0;32m    374\u001b[0m os\u001b[39m.\u001b[39mmakedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_dir_root, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    375\u001b[0m lock_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_dir_root, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_dir\u001b[39m.\u001b[39mreplace(os\u001b[39m.\u001b[39msep, \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.lock\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 376\u001b[0m \u001b[39mwith\u001b[39;00m FileLock(lock_path):\n\u001b[0;32m    377\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_dir):  \u001b[39m# check if data exist\u001b[39;00m\n\u001b[0;32m    378\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(os\u001b[39m.\u001b[39mlistdir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_dir)) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\froro\\OneDrive\\Escritorio\\UNAL\\RNA\\FinancIA\\env\\lib\\site-packages\\datasets\\utils\\filelock.py:320\u001b[0m, in \u001b[0;36mBaseFileLock.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 320\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    321\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\froro\\OneDrive\\Escritorio\\UNAL\\RNA\\FinancIA\\env\\lib\\site-packages\\datasets\\utils\\filelock.py:282\u001b[0m, in \u001b[0;36mBaseFileLock.acquire\u001b[1;34m(self, timeout, poll_intervall)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m             logger()\u001b[39m.\u001b[39mdebug(\n\u001b[0;32m    280\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLock \u001b[39m\u001b[39m{\u001b[39;00mlock_id\u001b[39m}\u001b[39;00m\u001b[39m not acquired on \u001b[39m\u001b[39m{\u001b[39;00mlock_filename\u001b[39m}\u001b[39;00m\u001b[39m, waiting \u001b[39m\u001b[39m{\u001b[39;00mpoll_intervall\u001b[39m}\u001b[39;00m\u001b[39m seconds ...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m             )\n\u001b[1;32m--> 282\u001b[0m             time\u001b[39m.\u001b[39;49msleep(poll_intervall)\n\u001b[0;32m    283\u001b[0m \u001b[39mexcept\u001b[39;00m:  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m    284\u001b[0m     \u001b[39m# Something did go wrong, so decrement the counter.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_thread_lock:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from pathlib import Path\n",
        "load_dataset(\"csv\",\"Dataset\\train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOD5WnybtT_I"
      },
      "outputs": [],
      "source": [
        "from torchmetrics import Accuracy, F1, Precision, Recall\n",
        "\n",
        "class FinanciaMultilabel(pl.LightningModule):\n",
        "    \"\"\"This class is used to create the model\"\"\"\n",
        "    def __init__(self, num_labels, model_name, class_weights):\n",
        "        super().__init__()\n",
        "        self.num_labels = num_labels\n",
        "        # The models is multi-label, so we need to use BCEWithLogitsLoss\n",
        "        self.loss = nn.BCEWithLogitsLoss(pos_weight=class_weights, reduction='none')\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=self.num_labels, ignore_mismatched_sizes=True)\n",
        "        self._tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        #Metrics for the model for training\n",
        "        self.train_f1 = F1(task = \"multilabel\", num_classes=self.num_labels)\n",
        "        self.train_accuracy = Accuracy(task =\"multilabel\", num_classes=self.num_labels)\n",
        "        self.train_precision = Precision(task =\"multilabel\", num_classes=self.num_labels)\n",
        "        self.train_recall = Recall(task =\"multilabel\", num_classes=self.num_labels)\n",
        "        # Metrics for the model for validation\n",
        "        self.val_f1 = F1(task = \"multilabel\", num_classes=self.num_labels)\n",
        "        self.val_accuracy = Accuracy(task =\"multilabel\", num_classes=self.num_labels)\n",
        "        self.val_precision = Precision(task =\"multilabel\", num_classes=self.num_labels)\n",
        "        self.val_recall = Recall(task =\"multilabel\", num_classes=self.num_labels)\n",
        "        \n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"This function is used to forward the data through the model\"\"\"\n",
        "        output = self.model(input_ids, attention_mask=attention_mask)\n",
        "        logits = output.logits\n",
        "        return logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"This function is used to train the model\"\"\"\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        logits = self(input_ids, attention_mask)\n",
        "        train_loss = self.loss(logits, labels)\n",
        "        train_acc = self.train_accuracy(logits, labels)\n",
        "        train_f1 = self.train_f1(logits, labels)\n",
        "        train_score = self.train_precision(logits, labels)\n",
        "        train_recall = self.train_recall(logits, labels)\n",
        "        self.log(\"train/loss\", loss, on_step=False, on_epoch=True)\n",
        "        self.log(\"train/accuracy\", self.train_accuracy, on_step=False, on_epoch=True)\n",
        "        self.log(\"train/precision\", self.train_precision, on_step=False, on_epoch=True)\n",
        "        self.log(\"train/recall\", self.train_recall, on_step=False, on_epoch=True)\n",
        "        self.log(\"train/f1_score\", self.train_f1_score, on_step=False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"This function is used to validate the model\"\"\"\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        logits = self(input_ids, attention_mask)\n",
        "        val_loss = self.loss(logits, labels)\n",
        "        val_acc = self.val_accuracy(outputs, labels)\n",
        "        val_f1 = self.val_f1(outputs, labels)\n",
        "        val_recall = self.val_recall(outputs, labels)\n",
        "        val_precision = self.val_precision(outputs, labels)\n",
        "        \n",
        "        self.log(\"val/loss\", val_loss, on_step=False, on_epoch=True)\n",
        "        self.log(\"val/accuracy\", self.val_accuracy, on_step=False, on_epoch=True)\n",
        "        self.log(\"val/precision\", self.val_precision, on_step=False, on_epoch=True)\n",
        "        self.log(\"val/recall\", self.val_recall, on_step=False, on_epoch=True)\n",
        "        self.log(\"val/f1_score\", self.val_f1_score, on_step=False, on_epoch=True)\n",
        "        self.log(\"val/loss\", val_loss, on_step=False, on_epoch=True)\n",
        "        return val_loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"This function is used to configure the optimizer\"\"\"\n",
        "        optimizer = torch.optim.AdamW\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}