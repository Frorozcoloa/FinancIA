{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import set_seed\n",
    "\n",
    "def SEED(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    set_seed(seed)\n",
    "SEED(42)\n",
    "\n",
    "\n",
    "# Configuration by model\n",
    "NUM_VARAIBLES = 3\n",
    "NUM_LABELS = 3\n",
    "num_labels = NUM_LABELS * NUM_VARAIBLES\n",
    "model_name = \"pysentimiento/roberta-es-sentiment\"\n",
    "\n",
    "divice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([9, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([9]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ( #\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    modelForSequenceClassification,\n",
    "    get_constant_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "# Configuring the model\n",
    "num_labels = NUM_LABELS * NUM_VARAIBLES\n",
    "# model_name = \"pysentimiento/roberta-es-sentiment\"\n",
    "model_name = \"pysentimiento/robertuito-sentiment-analysis\"\n",
    "auto_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[1;32m----> 3\u001b[0m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mcsv\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mDataset\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39mrain.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\froro\\OneDrive\\Escritorio\\UNAL\\RNA\\FinancIA\\env\\lib\\site-packages\\datasets\\load.py:1767\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1762\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[0;32m   1763\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[0;32m   1764\u001b[0m )\n\u001b[0;32m   1766\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 1767\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   1768\u001b[0m     path\u001b[39m=\u001b[39mpath,\n\u001b[0;32m   1769\u001b[0m     name\u001b[39m=\u001b[39mname,\n\u001b[0;32m   1770\u001b[0m     data_dir\u001b[39m=\u001b[39mdata_dir,\n\u001b[0;32m   1771\u001b[0m     data_files\u001b[39m=\u001b[39mdata_files,\n\u001b[0;32m   1772\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   1773\u001b[0m     features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m   1774\u001b[0m     download_config\u001b[39m=\u001b[39mdownload_config,\n\u001b[0;32m   1775\u001b[0m     download_mode\u001b[39m=\u001b[39mdownload_mode,\n\u001b[0;32m   1776\u001b[0m     revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   1777\u001b[0m     use_auth_token\u001b[39m=\u001b[39muse_auth_token,\n\u001b[0;32m   1778\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[0;32m   1779\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1780\u001b[0m )\n\u001b[0;32m   1782\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32mc:\\Users\\froro\\OneDrive\\Escritorio\\UNAL\\RNA\\FinancIA\\env\\lib\\site-packages\\datasets\\load.py:1524\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1521\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg)\n\u001b[0;32m   1523\u001b[0m \u001b[39m# Instantiate the dataset builder\u001b[39;00m\n\u001b[1;32m-> 1524\u001b[0m builder_instance: DatasetBuilder \u001b[39m=\u001b[39m builder_cls(\n\u001b[0;32m   1525\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   1526\u001b[0m     config_name\u001b[39m=\u001b[39mconfig_name,\n\u001b[0;32m   1527\u001b[0m     data_dir\u001b[39m=\u001b[39mdata_dir,\n\u001b[0;32m   1528\u001b[0m     data_files\u001b[39m=\u001b[39mdata_files,\n\u001b[0;32m   1529\u001b[0m     \u001b[39mhash\u001b[39m\u001b[39m=\u001b[39m\u001b[39mhash\u001b[39m,\n\u001b[0;32m   1530\u001b[0m     features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m   1531\u001b[0m     use_auth_token\u001b[39m=\u001b[39muse_auth_token,\n\u001b[0;32m   1532\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[0;32m   1533\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbuilder_kwargs,\n\u001b[0;32m   1534\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   1535\u001b[0m )\n\u001b[0;32m   1537\u001b[0m \u001b[39mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[1;32mc:\\Users\\froro\\OneDrive\\Escritorio\\UNAL\\RNA\\FinancIA\\env\\lib\\site-packages\\datasets\\builder.py:376\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[1;34m(self, cache_dir, config_name, hash, base_path, info, features, use_auth_token, repo_id, data_files, data_dir, storage_options, writer_batch_size, name, **config_kwargs)\u001b[0m\n\u001b[0;32m    374\u001b[0m os\u001b[39m.\u001b[39mmakedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_dir_root, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    375\u001b[0m lock_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_dir_root, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_dir\u001b[39m.\u001b[39mreplace(os\u001b[39m.\u001b[39msep, \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.lock\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 376\u001b[0m \u001b[39mwith\u001b[39;00m FileLock(lock_path):\n\u001b[0;32m    377\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_dir):  \u001b[39m# check if data exist\u001b[39;00m\n\u001b[0;32m    378\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(os\u001b[39m.\u001b[39mlistdir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_dir)) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\froro\\OneDrive\\Escritorio\\UNAL\\RNA\\FinancIA\\env\\lib\\site-packages\\datasets\\utils\\filelock.py:320\u001b[0m, in \u001b[0;36mBaseFileLock.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 320\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    321\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\froro\\OneDrive\\Escritorio\\UNAL\\RNA\\FinancIA\\env\\lib\\site-packages\\datasets\\utils\\filelock.py:282\u001b[0m, in \u001b[0;36mBaseFileLock.acquire\u001b[1;34m(self, timeout, poll_intervall)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m             logger()\u001b[39m.\u001b[39mdebug(\n\u001b[0;32m    280\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLock \u001b[39m\u001b[39m{\u001b[39;00mlock_id\u001b[39m}\u001b[39;00m\u001b[39m not acquired on \u001b[39m\u001b[39m{\u001b[39;00mlock_filename\u001b[39m}\u001b[39;00m\u001b[39m, waiting \u001b[39m\u001b[39m{\u001b[39;00mpoll_intervall\u001b[39m}\u001b[39;00m\u001b[39m seconds ...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m             )\n\u001b[1;32m--> 282\u001b[0m             time\u001b[39m.\u001b[39;49msleep(poll_intervall)\n\u001b[0;32m    283\u001b[0m \u001b[39mexcept\u001b[39;00m:  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m    284\u001b[0m     \u001b[39m# Something did go wrong, so decrement the counter.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_thread_lock:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "load_dataset(\"csv\",\"Dataset\\train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy, F1, Precision, Recall\n",
    "\n",
    "class FinanciaMultilabel(pl.LightningModule):\n",
    "    \"\"\"This class is used to create the model\"\"\"\n",
    "    def __init__(self, num_labels, model_name, class_weights):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        # The models is multi-label, so we need to use BCEWithLogitsLoss\n",
    "        self.loss = nn.BCEWithLogitsLoss(pos_weight=class_weights, reduction='none')\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=self.num_labels, ignore_mismatched_sizes=True)\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        #Metrics for the model for training\n",
    "        self.train_f1 = F1(task = \"multilabel\", num_classes=self.num_labels)\n",
    "        self.train_accuracy = Accuracy(task =\"multilabel\", num_classes=self.num_labels)\n",
    "        self.train_precision = Precision(task =\"multilabel\", num_classes=self.num_labels)\n",
    "        self.train_recall = Recall(task =\"multilabel\", num_classes=self.num_labels)\n",
    "        # Metrics for the model for validation\n",
    "        self.val_f1 = F1(task = \"multilabel\", num_classes=self.num_labels)\n",
    "        self.val_accuracy = Accuracy(task =\"multilabel\", num_classes=self.num_labels)\n",
    "        self.val_precision = Precision(task =\"multilabel\", num_classes=self.num_labels)\n",
    "        self.val_recall = Recall(task =\"multilabel\", num_classes=self.num_labels)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"This function is used to forward the data through the model\"\"\"\n",
    "        output = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = output.logits\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"This function is used to train the model\"\"\"\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        train_loss = self.loss(logits, labels)\n",
    "        train_acc = self.train_accuracy(logits, labels)\n",
    "        train_f1 = self.train_f1(logits, labels)\n",
    "        train_score = self.train_precision(logits, labels)\n",
    "        train_recall = self.train_recall(logits, labels)\n",
    "        self.log(\"train/loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"train/accuracy\", self.train_accuracy, on_step=False, on_epoch=True)\n",
    "        self.log(\"train/precision\", self.train_precision, on_step=False, on_epoch=True)\n",
    "        self.log(\"train/recall\", self.train_recall, on_step=False, on_epoch=True)\n",
    "        self.log(\"train/f1_score\", self.train_f1_score, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"This function is used to validate the model\"\"\"\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        val_loss = self.loss(logits, labels)\n",
    "        val_acc = self.val_accuracy(outputs, labels)\n",
    "        val_f1 = self.val_f1(outputs, labels)\n",
    "        val_recall = self.val_recall(outputs, labels)\n",
    "        val_precision = self.val_precision(outputs, labels)\n",
    "        \n",
    "        self.log(\"val/loss\", val_loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"val/accuracy\", self.val_accuracy, on_step=False, on_epoch=True)\n",
    "        self.log(\"val/precision\", self.val_precision, on_step=False, on_epoch=True)\n",
    "        self.log(\"val/recall\", self.val_recall, on_step=False, on_epoch=True)\n",
    "        self.log(\"val/f1_score\", self.val_f1_score, on_step=False, on_epoch=True)\n",
    "        self.log(\"val/loss\", val_loss, on_step=False, on_epoch=True)\n",
    "        return val_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"This function is used to configure the optimizer\"\"\"\n",
    "        optimizer = torch.optim.AdamW\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
