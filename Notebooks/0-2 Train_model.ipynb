{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Frorozcoloa/FinancIA/blob/main/Notebooks/0-2%20Train_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epwoT-bYtT_E",
        "outputId": "49014481-1f87-4fbb-901f-71a95a4277fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.14.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning\n",
            "  Downloading lightning-2.0.1.post0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.19.1-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (67.6.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers wandb torchmetrics lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import set_seed\n",
        "from torch import nn\n",
        "\n",
        "def SEED(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    set_seed(seed)\n",
        "\n",
        "\n",
        "SEED(42)\n",
        "\n",
        "\n",
        "# Configuration by model\n",
        "NUM_VARAIBLES = 3\n",
        "NUM_LABELS = 3\n",
        "num_labels = NUM_LABELS * NUM_VARAIBLES\n",
        "model_name = \"pysentimiento/roberta-es-sentiment\"\n",
        "\n",
        "divice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "HH7wdTX7tX8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eDkRBbStT_H"
      },
      "outputs": [],
      "source": [
        "from transformers import ( \n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    get_constant_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "# Configuring the model\n",
        "num_labels = NUM_LABELS * NUM_VARAIBLES\n",
        "# model_name = \"pysentimiento/roberta-es-sentiment\"\n",
        "model_name = \"pysentimiento/robertuito-sentiment-analysis\"\n",
        "auto_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, ignore_mismatched_sizes=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auto_tokenizer.encode_plus(\"Hola mundo\")"
      ],
      "metadata": {
        "id": "w3nFGjgtXfUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FinanciaSentimental(Dataset):\n",
        "    \"\"\"This class is used to load the data and tokenize it\"\"\"\n",
        "    def __init__(self, tokenizer, dataframe, columns):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dataframe = dataframe\n",
        "        ## Columns to target\n",
        "        self._columns = columns\n",
        "    \n",
        "    @property\n",
        "    def columns(self):\n",
        "        \"\"\"Return the columns to target\"\"\"\n",
        "        return self._columns\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the length of the dataset\"\"\"\n",
        "        return len(self.dataframe)\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Get the data at the index\"\"\"\n",
        "        values = self.dataframe.iloc[index]\n",
        "        text = values['text']\n",
        "        label = values[self._columns].values.astype(np.float32)\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        label = torch.tensor(label, dtype=torch.float32)\n",
        "        return inputs['input_ids'].squeeze(0), inputs['attention_mask'].squeeze(0), label"
      ],
      "metadata": {
        "id": "Lhl4Jh_1HT54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOD5WnybtT_I"
      },
      "outputs": [],
      "source": [
        "from torchmetrics import Accuracy, F1Score, Precision, Recall\n",
        "import lightning.pytorch as pl\n",
        "\n",
        "class FinanciaMultilabel(pl.LightningModule):\n",
        "    \"\"\"This class is used to create the model\"\"\"\n",
        "    def __init__(self, num_labels, model_name, class_weights):\n",
        "        super().__init__()\n",
        "        self.num_labels = num_labels\n",
        "        # The models is multi-label, so we need to use BCEWithLogitsLoss\n",
        "        self.loss = nn.BCEWithLogitsLoss(pos_weight=class_weights, reduction='none')\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=self.num_labels, ignore_mismatched_sizes=True)\n",
        "        self._tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        #Metrics for the model for training\n",
        "        self.train_f1 = F1Score(task = \"multilabel\", num_labels=num_labels)\n",
        "        self.train_accuracy = Accuracy(task =\"multilabel\", num_labels=num_labels)\n",
        "        self.train_precision = Precision(task =\"multilabel\", num_labels=num_labels)\n",
        "        self.train_recall = Recall(task =\"multilabel\", num_labels=num_labels)\n",
        "        # Metrics for the model for validation\n",
        "        self.val_f1 = F1Score(task = \"multilabel\", num_labels=num_labels)\n",
        "        self.val_accuracy = Accuracy(task =\"multilabel\", num_labels=num_labels)\n",
        "        self.val_precision = Precision(task =\"multilabel\", num_labels=num_labels)\n",
        "        self.val_recall = Recall(task =\"multilabel\", num_labels=num_labels)\n",
        "        \n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"This function is used to forward the data through the model\"\"\"\n",
        "        output = self.model(input_ids, attention_mask=attention_mask)\n",
        "        logits = output.logits\n",
        "        return logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"This function is used to train the model\"\"\"\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        logits = self(input_ids, attention_mask)\n",
        "        train_loss = self.loss(logits, labels)\n",
        "        train_acc = self.train_accuracy(logits, labels)\n",
        "        train_f1 = self.train_f1(logits, labels)\n",
        "        train_score = self.train_precision(logits, labels)\n",
        "        train_recall = self.train_recall(logits, labels)\n",
        "        self.log(\"train/loss\", train_loss, on_step=False, on_epoch=True)\n",
        "        self.log(\"train/accuracy\", self.train_accuracy, on_step=False, on_epoch=True)\n",
        "        self.log(\"train/precision\", self.train_precision, on_step=False, on_epoch=True)\n",
        "        self.log(\"train/recall\", self.train_recall, on_step=False, on_epoch=True)\n",
        "        self.log(\"train/f1_score\", self.train_f1_score, on_step=False, on_epoch=True)\n",
        "        return train_loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"This function is used to validate the model\"\"\"\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        logits = self(input_ids, attention_mask)\n",
        "        val_loss = self.loss(logits, labels)\n",
        "        val_acc = self.val_accuracy(logits, labels)\n",
        "        val_f1 = self.val_f1(logits, labels)\n",
        "        val_recall = self.val_recall(logits, labels)\n",
        "        val_precision = self.val_precision(logits, labels)\n",
        "        \n",
        "        self.log(\"val/loss\", val_loss, on_step=False, on_epoch=True)\n",
        "        self.log(\"val/accuracy\", self.val_accuracy, on_step=False, on_epoch=True)\n",
        "        self.log(\"val/precision\", self.val_precision, on_step=False, on_epoch=True)\n",
        "        self.log(\"val/recall\", self.val_recall, on_step=False, on_epoch=True)\n",
        "        self.log(\"val/f1_score\", self.val_f1_score, on_step=False, on_epoch=True)\n",
        "        self.log(\"val/loss\", val_loss, on_step=False, on_epoch=True)\n",
        "        return val_loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"This function is used to configure the optimizer\"\"\"\n",
        "        optimizer = torch.optim.AdamW(self.parameters())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mm0B6GdXKnqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = \"/content/drive/Shareddrives/Redes neuronales/Datasets/df_with_sentiment.csv\"\n",
        "df = pd.read_csv(train_df)\n",
        "df = df[[\"id\",\t\"text\",\t\"target\",\t\"target_sentiment\",\t\"companies_sentiment\",\t\"consumers_sentiment\", \"tag\"]]\n",
        "df = pd.get_dummies(df, columns = [\"target_sentiment\",\t\"companies_sentiment\",\"consumers_sentiment\"])\n",
        "df_train = df[df.tag == \"train\"]\n",
        "df_test = df[df.tag == \"test\"]\n",
        "df_valid, df_test = train_test_split(df_test, test_size=0.5)"
      ],
      "metadata": {
        "id": "_KZEVfRKJ_ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid"
      ],
      "metadata": {
        "id": "ObWAUFqAWj-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_varaibles = [\"target_sentiment_negative\",\t\"target_sentiment_neutral\",\t\"target_sentiment_positive\"\t,\"companies_sentiment_negative\"\t,\"companies_sentiment_neutral\",\t\"companies_sentiment_positive\", 'consumers_sentiment_negative',\n",
        "       'consumers_sentiment_neutral', 'consumers_sentiment_positive']"
      ],
      "metadata": {
        "id": "-OYz0z0gV1Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)\n",
        "print(df_train.shape)"
      ],
      "metadata": {
        "id": "6m6SFr7gUNgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = FinanciaSentimental(auto_tokenizer, df_train, columns_varaibles)\n",
        "valid_dataset = FinanciaSentimental(auto_tokenizer, df_valid, columns_varaibles)\n",
        "test_dataset = FinanciaSentimental(auto_tokenizer, df_test, columns_varaibles)"
      ],
      "metadata": {
        "id": "UreLqvPsKY0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input, attention, mask = next(iter(train_dataset))\n",
        "print(input.shape)\n",
        "print(attention.shape)\n",
        "print(mask.shape)"
      ],
      "metadata": {
        "id": "WA-K7IN0XgSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "uLZT5I9BU-76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input, attention, mask = next(iter(train_dataloader))\n",
        "print(input.shape)\n",
        "print(attention.shape)\n",
        "print(mask.shape)"
      ],
      "metadata": {
        "id": "7sH1AmiSY2IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weight = torch.ones(9)\n",
        "model = FinanciaMultilabel(9, model_name,class_weight )"
      ],
      "metadata": {
        "id": "umPhOkH5Y7XR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.loggers import WandbLogger\n",
        "wandb_logger = WandbLogger(project='FinancIA', name='#1', save_code=True, log_model=False, sync_tensorboard=True, save_dir=\"./logs\")\n",
        "wandb_logger.watch(model, log=\"all\")"
      ],
      "metadata": {
        "id": "w-NGzgxTbfaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(monitor=\"val/f1_score\", mode=\"max\", save_last=True, save_weights_only=True)\n",
        "tqdm_callback = TQDMProgressBar(refresh_rate=1)\n",
        "early_stop_callback = EarlyStopping(monitor='val_loss', patience=5)\n",
        "trainer = pl.Trainer( accelerator=\"cuda\", max_epochs=10, logger=wandb_logger, callbacks=[checkpoint_callback, tqdm_callback,early_stop_callback], precision=16,)\n",
        "trainer.fit(model, train_dataloader, valid_dataloader)"
      ],
      "metadata": {
        "id": "Qbcxr5m3cgZm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}